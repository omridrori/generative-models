{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled18.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQSgD6lIUxAJMy3bEpjzur",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omridrori/generative-models/blob/main/gan%20models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ew_cSEPsTFVr"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GANLossGenerator(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        super(GANLossGenerator, self).__init__()\n",
        "\n",
        "    def forward(self, discriminator_prediction_fake: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "\n",
        "        # Loss can be computed by utilizing the softplus function since softplus combines both sigmoid and log\n",
        "        return - F.softplus(discriminator_prediction_fake).mean()\n",
        "\n",
        "\n",
        "class GANLossDiscriminator(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        # Call super constructor\n",
        "        super(GANLossDiscriminator, self).__init__()\n",
        "\n",
        "    def forward(self, discriminator_prediction_real: torch.Tensor,\n",
        "                discriminator_prediction_fake: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "\n",
        "        # Loss can be computed by utilizing the softplus function since softplus combines both sigmoid and log\n",
        "        return F.softplus(- discriminator_prediction_real).mean() \\\n",
        "               + F.softplus(discriminator_prediction_fake).mean()\n",
        "\n",
        "\n",
        "class NSGANLossGenerator(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        super(NSGANLossGenerator, self).__init__()\n",
        "\n",
        "    def forward(self, discriminator_prediction_fake: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "\n",
        "        # Loss can be computed by utilizing the softplus function since softplus combines both sigmoid and log\n",
        "        return F.softplus(- discriminator_prediction_fake).mean()\n",
        "\n",
        "\n",
        "class NSGANLossDiscriminator(GANLossDiscriminator):\n",
        "\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        super(NSGANLossDiscriminator, self).__init__()\n",
        "\n",
        "\n",
        "class WassersteinGANLossGenerator(nn.Module):\n",
        "\n",
        "    def __index__(self) -> None:\n",
        "\n",
        "        # Call super constructor\n",
        "        super(WassersteinGANLossGenerator, self).__index__()\n",
        "\n",
        "    def forward(self, discriminator_prediction_fake: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "\n",
        "        return - discriminator_prediction_fake.mean()\n",
        "\n",
        "\n",
        "class WassersteinGANLossDiscriminator(nn.Module):\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        super(WassersteinGANLossDiscriminator, self).__init__()\n",
        "\n",
        "    def forward(self, discriminator_prediction_real: torch.Tensor,\n",
        "                discriminator_prediction_fake: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "\n",
        "        return - discriminator_prediction_real.mean() \\\n",
        "               + discriminator_prediction_fake.mean()\n",
        "\n",
        "\n",
        "class WassersteinGANLossGPGenerator(WassersteinGANLossGenerator):\n",
        "\n",
        "    def __index__(self) -> None:\n",
        "\n",
        "        super(WassersteinGANLossGPGenerator, self).__index__()\n",
        "\n",
        "\n",
        "class WassersteinGANLossGPDiscriminator(nn.Module):\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        super(WassersteinGANLossGPDiscriminator, self).__init__()\n",
        "\n",
        "    def forward(self, discriminator_prediction_real: torch.Tensor,\n",
        "                discriminator_prediction_fake: torch.Tensor,\n",
        "                discriminator: nn.Module,\n",
        "                real_samples: torch.Tensor,\n",
        "                fake_samples: torch.Tensor,\n",
        "                lambda_gradient_penalty: Optional[float] = 2., **kwargs) -> torch.Tensor:\n",
        "\n",
        "        # Generate random alpha for interpolation\n",
        "        alpha = torch.rand((real_samples.shape[0], 1), device=real_samples.device)\n",
        "        # Make interpolated samples\n",
        "        samples_interpolated = (alpha * real_samples + (1. - alpha) * fake_samples)\n",
        "        samples_interpolated.requires_grad = True\n",
        "        # Make discriminator prediction\n",
        "        discriminator_prediction_interpolated = discriminator(samples_interpolated)\n",
        "        # Calc gradients\n",
        "        gradients = torch.autograd.grad(outputs=discriminator_prediction_interpolated.sum(),\n",
        "                                        inputs=samples_interpolated,\n",
        "                                        create_graph=True,\n",
        "                                        retain_graph=True)[0]\n",
        "        # Calc gradient penalty\n",
        "        gradient_penalty = (gradients.view(gradients.shape[0], -1).norm(dim=1) - 1.).pow(2).mean()\n",
        "        return - discriminator_prediction_real.mean() \\\n",
        "               + discriminator_prediction_fake.mean() \\\n",
        "               + lambda_gradient_penalty * gradient_penalty\n",
        "\n",
        "\n",
        "class LSGANLossGenerator(nn.Module):\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        super(LSGANLossGenerator, self).__init__()\n",
        "\n",
        "    def forward(self, discriminator_prediction_fake: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "\n",
        "        return - 0.5 * (discriminator_prediction_fake - 1.).pow(2).mean()\n",
        "\n",
        "\n",
        "class LSGANLossDiscriminator(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        # Call super constructor\n",
        "        super(LSGANLossDiscriminator, self).__init__()\n",
        "\n",
        "    def forward(self, discriminator_prediction_real: torch.Tensor,\n",
        "                discriminator_prediction_fake: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "\n",
        "        return 0.5 * ((- discriminator_prediction_real - 1.).pow(2).mean()\n",
        "                      + discriminator_prediction_fake.pow(2).mean())\n",
        "\n",
        "\n",
        "class HingeGANLossGenerator(WassersteinGANLossGenerator):\n",
        "\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"\n",
        "        Constructor method.\n",
        "        \"\"\"\n",
        "        # Call super constructor\n",
        "        super(HingeGANLossGenerator, self).__init__()\n",
        "\n",
        "\n",
        "class HingeGANLossDiscriminator(nn.Module):\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        super(HingeGANLossDiscriminator, self).__init__()\n",
        "\n",
        "    def forward(self, discriminator_prediction_real: torch.Tensor,\n",
        "                discriminator_prediction_fake: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "\n",
        "        return - torch.minimum(torch.tensor(0., dtype=torch.float, device=discriminator_prediction_real.device),\n",
        "                               discriminator_prediction_real - 1.).mean() \\\n",
        "               - torch.minimum(torch.tensor(0., dtype=torch.float, device=discriminator_prediction_fake.device),\n",
        "                               - discriminator_prediction_fake - 1.).mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rGRxYawuWAxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import spectral_norm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_generator(latent_size: int) -> nn.Module:\n",
        "\n",
        "    return nn.Sequential(nn.Linear(latent_size, 256, bias=True),\n",
        "                         nn.LeakyReLU(),\n",
        "                         nn.Linear(256, 256, bias=True),\n",
        "                         nn.LeakyReLU(),\n",
        "                         nn.Linear(256, 256, bias=True),\n",
        "                         nn.LeakyReLU(),\n",
        "                         nn.Linear(256, 256, bias=True),\n",
        "                         nn.Tanh(),\n",
        "                         nn.Linear(256, 2, bias=True))\n",
        "\n",
        "\n",
        "def get_discriminator(use_spectral_norm: bool) -> nn.Module:\n",
        "\n",
        "    if use_spectral_norm:\n",
        "        return nn.Sequential(spectral_norm(nn.Linear(2, 256, bias=True)),\n",
        "                             nn.LeakyReLU(),\n",
        "                             spectral_norm(nn.Linear(256, 256, bias=True)),\n",
        "                             nn.LeakyReLU(),\n",
        "                             spectral_norm(nn.Linear(256, 256, bias=True)),\n",
        "                             nn.LeakyReLU(),\n",
        "                             spectral_norm(nn.Linear(256, 256, bias=True)),\n",
        "                             nn.LeakyReLU(),\n",
        "                             spectral_norm(nn.Linear(256, 1, bias=True)))\n",
        "    return nn.Sequential(nn.Linear(2, 256, bias=True),\n",
        "                         nn.LeakyReLU(),\n",
        "                         nn.Linear(256, 256, bias=True),\n",
        "                         nn.LeakyReLU(),\n",
        "                         nn.Linear(256, 256, bias=True),\n",
        "                         nn.LeakyReLU(),\n",
        "                         nn.Linear(256, 256, bias=True),\n",
        "                         nn.LeakyReLU(),\n",
        "                         nn.Linear(256, 1, bias=True))\n",
        "\n",
        "\n",
        "def get_data(samples: Optional[int] = 400, variance: Optional[float] = 0.05) -> torch.Tensor:\n",
        "\n",
        "    assert samples % 8 == 0 and samples > 0, \"Number of samples must be a multiple of 8 and bigger than 0\"\n",
        "    # Init angels of the means\n",
        "    angels = torch.cumsum((2 * np.pi / 8) * torch.ones((8)), dim=0)\n",
        "    # Convert angles to 2D coordinates\n",
        "    means = torch.stack([torch.cos(angels), torch.sin(angels)], dim=0)\n",
        "    # Generate data\n",
        "    data = torch.empty((2, samples))\n",
        "    counter = 0\n",
        "    for gaussian in range(means.shape[1]):\n",
        "        for sample in range(int(samples / 8)):\n",
        "            data[:, counter] = torch.normal(means[:, gaussian], variance)\n",
        "            counter += 1\n",
        "    # Reshape data\n",
        "    data = data.T\n",
        "    # Shuffle data\n",
        "    data = data[torch.randperm(data.shape[0])]\n",
        "    # Convert numpy array to tensor\n",
        "    return data.float()"
      ],
      "metadata": {
        "id": "FPyDLr_XTz7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device='cuda'\n",
        "epochs=500\n",
        "d_updates=1\n",
        "plot_frequency=10\n",
        "lr=0.0001\n",
        "latent_size=32\n",
        "samples=10000\n",
        "batch_size=500\n",
        "loss='hinge'#['standard', 'non-saturating', 'hinge', 'wasserstein', 'wasserstein-gp', 'least-squares']\n",
        "spectral_norm='false'\n",
        "clip_weights=1\n",
        "topk=True"
      ],
      "metadata": {
        "id": "a3rM8w_rWAMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get arguments\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Make directory to save plots\n",
        "    path = os.path.join(os.getcwd(), 'plots', loss + (\"_top_k\" if topk else \"\") + (\"_sn\" if spectral_norm else \"\") + (\"_clip\" if clip_weights else \"\"))\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    # Init hyperparameters\n",
        "    fixed_generator_noise: torch.Tensor = torch.randn([samples // 10, latent_size], device=device)\n",
        "    # Get data\n",
        "    data: torch.Tensor = get_data(samples=samples).to(device)\n",
        "    # Get generator\n",
        "    generator: nn.Module = get_generator(latent_size=latent_size)\n",
        "    # Get discriminator\n",
        "    discriminator: nn.Module = get_discriminator(use_spectral_norm=spectral_norm)\n",
        "    # Init Loss function\n",
        "    if loss == 'standard':\n",
        "        loss_generator: nn.Module = GANLossGenerator()\n",
        "        loss_discriminator: nn.Module = GANLossDiscriminator()\n",
        "    elif loss == 'non-saturating':\n",
        "        loss_generator: nn.Module = NSGANLossGenerator()\n",
        "        loss_discriminator: nn.Module = NSGANLossDiscriminator()\n",
        "    elif loss == 'hinge':\n",
        "        loss_generator: nn.Module = HingeGANLossGenerator()\n",
        "        loss_discriminator: nn.Module = HingeGANLossDiscriminator()\n",
        "    elif loss == 'wasserstein':\n",
        "        loss_generator: nn.Module = WassersteinGANLossGenerator()\n",
        "        loss_discriminator: nn.Module = WassersteinGANLossDiscriminator()\n",
        "    elif loss == 'wasserstein-gp':\n",
        "        loss_generator: nn.Module = WassersteinGANLossGPGenerator()\n",
        "        loss_discriminator: nn.Module = WassersteinGANLossGPDiscriminator()\n",
        "    else:\n",
        "        loss_generator: nn.Module = LSGANLossGenerator()\n",
        "        loss_discriminator: nn.Module = LSGANLossDiscriminator()\n",
        "    # Networks to train mode\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    # Models to device\n",
        "    generator.to(device)\n",
        "    discriminator.to(device)\n",
        "    # Init optimizer\n",
        "    generator_optimizer: torch.optim.Optimizer = torch.optim.RMSprop(generator.parameters(), lr=lr)\n",
        "    discriminator_optimizer: torch.optim.Optimizer = torch.optim.RMSprop(discriminator.parameters(), lr=lr)\n",
        "    # Init progress bar\n",
        "    progress_bar = tqdm(total=epochs)\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):  # type: int\n",
        "        # Update progress bar\n",
        "        progress_bar.update(n=1)\n",
        "        # Update discriminator more often than generator to train it till optimality and get more reliable gradients of Wasserstein\n",
        "        for _ in range(d_updates):  # type: int\n",
        "            # Shuffle data\n",
        "            data = data[torch.randperm(data.shape[0], device=device)]\n",
        "            for index in range(0, samples, batch_size):  # type:int\n",
        "                # Get batch\n",
        "                batch: torch.Tensor = data[index:index + batch_size]\n",
        "                # Get noise for generator\n",
        "                noise: torch.Tensor = torch.randn([batch_size, latent_size], device=device)\n",
        "                # Optimize discriminator\n",
        "                discriminator_optimizer.zero_grad()\n",
        "                generator_optimizer.zero_grad()\n",
        "                with torch.no_grad():\n",
        "                    fake_samples: torch.Tensor = generator(noise)\n",
        "                prediction_real: torch.Tensor = discriminator(batch)\n",
        "                prediction_fake: torch.Tensor = discriminator(fake_samples)\n",
        "                if isinstance(loss_discriminator, WassersteinGANLossGPDiscriminator):\n",
        "                    loss_d: torch.Tensor = loss_discriminator(prediction_real, prediction_fake, discriminator, batch,\n",
        "                                                            fake_samples)\n",
        "                else:\n",
        "                    loss_d: torch.Tensor = loss_discriminator(prediction_real, prediction_fake)\n",
        "                loss_d.backward()\n",
        "                discriminator_optimizer.step()\n",
        "\n",
        "                # Clip weights to enforce Lipschitz constraint as proposed in Wasserstein GAN paper\n",
        "                if clip_weights > 0:\n",
        "                    with torch.no_grad():\n",
        "                        for param in discriminator.parameters():\n",
        "                            param.clamp_(-clip_weights, clip_weights)\n",
        "\n",
        "            # Get noise for generator\n",
        "            noise: torch.Tensor = torch.randn([batch_size, latent_size], device=device)\n",
        "            # Optimize generator\n",
        "            discriminator_optimizer.zero_grad()\n",
        "            generator_optimizer.zero_grad()\n",
        "            fake_samples: torch.Tensor = generator(noise)\n",
        "            prediction_fake: torch.Tensor = discriminator(fake_samples)\n",
        "            if topk and (epoch >= 0.5 * epochs):\n",
        "                prediction_fake = torch.topk(input=prediction_fake[:, 0], k=prediction_fake.shape[0] // 2)[0]\n",
        "            loss_g: torch.Tensor = loss_generator(prediction_fake)\n",
        "            loss_g.backward()\n",
        "            generator_optimizer.step()\n",
        "            # Update progress bar description\n",
        "            progress_bar.set_description(\n",
        "                'Epoch {}, Generator loss {:.4f}, Discriminator loss {:.4f}'.format(epoch, loss_g.item(),\n",
        "                                                                                    loss_d.item()))\n",
        "        # Plot samples of generator\n",
        "        if ((epoch + 1) % plot_frequency) == 0:\n",
        "            generator.eval()\n",
        "            generator_samples = generator(fixed_generator_noise)\n",
        "            generator_samples = generator_samples.cpu().detach().numpy()\n",
        "            plt.scatter(data[::10, 0].cpu(), data[::10, 1].cpu(), color='blue', label='Samples from $p_{data}$', s=2, alpha=0.5)\n",
        "            plt.scatter(generator_samples[:, 0], generator_samples[:, 1], color='red',\n",
        "                        label='Samples from generator $G$', s=2, alpha=0.5)\n",
        "            plt.legend(loc=1)\n",
        "            plt.title('Step {}'.format((epoch + 1) * samples // batch_size))\n",
        "            plt.xlim((-1.5, 1.5))\n",
        "            plt.ylim((-1.5, 1.75))\n",
        "            plt.grid()\n",
        "            plt.savefig(os.path.join(path, '{}.png'.format(str(epoch + 1).zfill(4))))\n",
        "            plt.close()\n",
        "            generator.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA0wfCY2UyQy",
        "outputId": "424579c7-3d32-4763-f939-e034e42f383e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 499, Generator loss -0.8292, Discriminator loss 1.9206: 100%|██████████| 500/500 [01:42<00:00,  5.28it/s]"
          ]
        }
      ]
    }
  ]
}